#!/bin/bash

# ğŸ†“ FREE LOCAL AI SETUP SCRIPT
# Sets up Ollama + TinyLlama for instant, free medical AI

echo "ğŸ†“ Setting up FREE Local AI for Alt-X Assistant"
echo "================================================"
echo ""

# Check if Ollama is installed
if command -v ollama &> /dev/null; then
    echo "âœ… Ollama is already installed"
else
    echo "âŒ Ollama not found. Installing..."
    curl -fsSL https://ollama.com/install.sh | sh
    echo "âœ… Ollama installed successfully"
fi

echo ""
echo "ğŸ“¦ Checking available models..."

# Check if TinyLlama is available (fast, 637MB)
if ollama list | grep -q "tinyllama"; then
    echo "âœ… TinyLlama already downloaded (fast model)"
else
    echo "ğŸ“¥ Downloading TinyLlama (637MB, takes 2-5 min)..."
    ollama pull tinyllama
    echo "âœ… TinyLlama downloaded"
fi

# Check if Llama2 is available (better quality, 3.8GB)
if ollama list | grep -q "llama2"; then
    echo "âœ… Llama 2 already downloaded (high quality model)"
else
    echo ""
    read -p "ğŸ“¥ Download Llama 2 for better quality? (3.8GB, 10-20 min) [y/N]: " -n 1 -r
    echo
    if [[ $REPLY =~ ^[Yy]$ ]]; then
        ollama pull llama2
        echo "âœ… Llama 2 downloaded"
    else
        echo "â­ï¸  Skipped Llama 2 (you can download it later with: ollama pull llama2)"
    fi
fi

echo ""
echo "ğŸš€ Starting Ollama server..."

# Check if Ollama is already running
if lsof -Pi :11434 -sTCP:LISTEN -t >/dev/null; then
    echo "âœ… Ollama server already running on port 11434"
else
    # Start Ollama in background
    ollama serve > /dev/null 2>&1 &
    sleep 3
    echo "âœ… Ollama server started on port 11434"
fi

echo ""
echo "ğŸ§ª Testing FREE AI..."

# Test with a simple query
RESPONSE=$(curl -s -X POST http://localhost:11434/api/generate \
  -d '{"model":"tinyllama","prompt":"What is antibiotic resistance? Answer in one sentence.","stream":false}' \
  --max-time 10)

if [ $? -eq 0 ]; then
    echo "âœ… FREE Local AI is working!"
    echo ""
    echo "ğŸ“Š Test Response:"
    echo "$RESPONSE" | python3 -c "import sys, json; data=json.load(sys.stdin); print('   ' + data.get('response', 'No response')[:150] + '...')" 2>/dev/null || echo "   $RESPONSE" | head -c 150
else
    echo "âš ï¸  AI test failed, but setup is complete"
fi

echo ""
echo "================================================"
echo "ğŸ‰ FREE Local AI Setup Complete!"
echo "================================================"
echo ""
echo "ğŸ’° Cost: \$0/month (100% FREE)"
echo "ğŸ”’ Privacy: 100% local, no data leaves your Mac"
echo "âš¡ Speed: ~3 seconds per query (TinyLlama)"
echo "ğŸ“ˆ Quality: Good for medical queries"
echo ""
echo "ğŸ¯ Your Alt-X AI now uses:"
echo "   1ï¸âƒ£  Knowledge Base (instant, 15+ medications)"
echo "   2ï¸âƒ£  Local AI (TinyLlama, 3 sec responses)"
echo "   3ï¸âƒ£  Ollama API (localhost:11434)"
echo ""
echo "ğŸ”§ To switch to higher quality Llama 2:"
echo "   Edit: server/routes/lumina-ai-local.js"
echo "   Change: DEFAULT_MODEL = 'tinyllama' â†’ 'llama2'"
echo "   Trade-off: Better answers, slower (30-80 seconds)"
echo ""
echo "ğŸš€ Ready to use! Your backend is already configured."
echo "================================================"
