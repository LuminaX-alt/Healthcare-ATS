# üÜì FREE AI MODELS FOR ALT-X

## ‚úÖ CURRENTLY INSTALLED (You already have these!)

Check what you have:
```bash
ollama list
```

You should see:
- **llama2** ‚úÖ (Currently active in Alt-X)
- **mistral** ‚úÖ 
- **tinyllama** ‚úÖ

---

## üî• TOP FREE AI MODELS YOU CAN INSTALL

### 1. **Llama 3** (NEWEST, BEST!) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
```bash
ollama pull llama3
```
- **Size**: 4.7 GB
- **Quality**: EXCELLENT (better than Llama 2)
- **Speed**: ~5-10 seconds
- **Best for**: Everything! Smarter than Llama 2

### 2. **Phi-3** (Microsoft Medical AI) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
```bash
ollama pull phi3
```
- **Size**: 2.3 GB
- **Quality**: EXCELLENT for medical
- **Speed**: ~3-5 seconds (FAST!)
- **Best for**: Medical questions, clinical advice

### 3. **Mistral** (You already have!) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
```bash
ollama pull mistral
```
- **Size**: 4.4 GB
- **Quality**: EXCELLENT
- **Speed**: ~5-10 seconds
- **Best for**: Reasoning, logic, complex questions

### 4. **Gemma** (Google's AI) ‚≠ê‚≠ê‚≠ê‚≠ê
```bash
ollama pull gemma:7b
```
- **Size**: 5 GB
- **Quality**: EXCELLENT
- **Speed**: ~5-10 seconds
- **Best for**: General knowledge, explanations

### 5. **Neural Chat** (Intel's AI) ‚≠ê‚≠ê‚≠ê‚≠ê
```bash
ollama pull neural-chat
```
- **Size**: 4.1 GB
- **Quality**: GOOD for conversations
- **Speed**: ~5-8 seconds
- **Best for**: Natural conversations

### 6. **Llama 3.1** (Latest Meta AI) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
```bash
ollama pull llama3.1
```
- **Size**: 4.7 GB
- **Quality**: BEST OVERALL
- **Speed**: ~5-10 seconds
- **Best for**: Everything! Most advanced

### 7. **Qwen** (Alibaba's AI) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
```bash
ollama pull qwen:7b
```
- **Size**: 4.5 GB
- **Quality**: EXCELLENT
- **Speed**: ~5-10 seconds
- **Best for**: Medical knowledge, multilingual

---

## üöÄ RECOMMENDED FOR DOCTORS

**Option 1: BEST QUALITY (Recommended)**
```bash
ollama pull llama3.1
```
Then change model in code to: `'llama3.1'`

**Option 2: MEDICAL FOCUSED**
```bash
ollama pull phi3
```
Then change model in code to: `'phi3'`

**Option 3: FASTEST**
```bash
# You already have this!
```
Use: `'tinyllama'` (already installed)

---

## üíª HOW TO INSTALL A NEW MODEL

1. **Pull the model:**
```bash
ollama pull llama3.1
```

2. **Wait for download** (will take 2-10 minutes depending on size)

3. **Update Alt-X to use it:**

Open: `/server/routes/lumina-ai-local.js`

Change line 11:
```javascript
const DEFAULT_MODEL = 'llama3.1';  // ‚Üê Change this to your model
```

4. **Restart backend:**
```bash
cd /Users/mrdevsharma/Downloads/EX/healthcare-prototype
lsof -ti:3001 | xargs kill -9
cd server && node index.js &
```

---

## üéØ QUICK COMPARISON

| Model | Size | Quality | Speed | Best For |
|-------|------|---------|-------|----------|
| **Llama 3.1** | 4.7GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Medium | Everything (BEST!) |
| **Phi-3** | 2.3GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Fast | Medical questions |
| **Mistral** | 4.4GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Medium | Reasoning |
| **Llama 2** | 3.8GB | ‚≠ê‚≠ê‚≠ê‚≠ê | Medium | General (current) |
| **Gemma** | 5GB | ‚≠ê‚≠ê‚≠ê‚≠ê | Medium | General knowledge |
| **TinyLlama** | 637MB | ‚≠ê‚≠ê‚≠ê | Very Fast | Quick answers |

---

## üè• RECOMMENDATION FOR YOUR HOSPITAL

**Install Llama 3.1** - It's the newest and best!

```bash
# 1. Install it
ollama pull llama3.1

# 2. Edit the model file
nano /Users/mrdevsharma/Downloads/EX/healthcare-prototype/server/routes/lumina-ai-local.js

# Change line 11 to:
const DEFAULT_MODEL = 'llama3.1';

# 3. Restart
cd /Users/mrdevsharma/Downloads/EX/healthcare-prototype
lsof -ti:3001 | xargs kill -9
cd server && node index.js &
```

---

## ‚ö° CURRENT MODEL (ACTIVE NOW)

**Model**: Llama 2  
**Status**: ‚úÖ Working  
**To change**: See instructions above

---

## üîß TEST A MODEL

```bash
curl -X POST http://localhost:11434/api/generate \
  -d '{"model":"llama3.1","prompt":"What is diabetes?","stream":false}'
```

---

## üìö ALL AVAILABLE MODELS

See full list:
```bash
ollama list    # Your installed models
```

Browse online:
https://ollama.com/library

---

**YOU ARE THE BOSS! Pick any model you want! ü´°**
