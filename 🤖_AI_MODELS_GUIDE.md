# ü§ñ AI MODELS IN ALT-X - COMPLETE GUIDE

## ‚úÖ WHAT YOU HAVE NOW

Alt-X uses **REAL PRE-TRAINED AI MODELS** via Ollama - these are FREE, POWERFUL models that can answer **ANY question**!

---

## üì¶ INSTALLED AI MODELS

You currently have **3 FREE AI models** installed:

### 1. üî• **Llama 2** (7B) - **CURRENTLY ACTIVE**
- **Size**: 3.8 GB
- **Quality**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (EXCELLENT)
- **Speed**: ~5-10 seconds per response
- **Best for**: Complex medical questions, detailed explanations, general knowledge
- **Can answer**: Medical questions, science, coding, general knowledge, everything!

### 2. ‚ö° **Mistral** (7B) 
- **Size**: 4.4 GB
- **Quality**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (EXCELLENT)
- **Speed**: ~5-10 seconds per response
- **Best for**: Reasoning, problem-solving, technical questions
- **Can answer**: Everything! Very good at logic and explanations

### 3. üöÄ **TinyLlama** (1B)
- **Size**: 637 MB
- **Quality**: ‚≠ê‚≠ê‚≠ê (GOOD)
- **Speed**: ~2-3 seconds per response
- **Best for**: Quick answers, simple questions
- **Can answer**: Basic medical info, simple questions

---

## üîÑ HOW TO SWITCH MODELS

Open: `/server/routes/lumina-ai-local.js`

Change line 11:
```javascript
const DEFAULT_MODEL = 'llama2';  // ‚Üê Change this
```

**Options:**
- `'llama2'` - Best quality, slower (RECOMMENDED)
- `'mistral'` - Alternative high-quality model
- `'tinyllama'` - Fast but less accurate

Then restart the backend server!

---

## üÜì HOW TO ADD MORE FREE AI MODELS

You can install any model from Ollama's library:

### Popular Medical AI Models:
```bash
# Llama 3 (newer, better than Llama 2)
ollama pull llama3

# Phi-3 (Microsoft's medical-focused model)
ollama pull phi3

# Gemma (Google's open model)
ollama pull gemma:7b

# Code Llama (for coding help)
ollama pull codellama
```

### After installing, update the model name in the code:
```javascript
const DEFAULT_MODEL = 'llama3';  // Use the new model
```

---

## üß™ TEST YOUR AI MODEL

Run this command to test:
```bash
curl -X POST http://localhost:3001/api/lumina-ai-local/query \
  -H "Content-Type: application/json" \
  -d '{"query":"What is diabetes? Explain in simple terms."}'
```

---

## üí° WHAT CAN ALT-X AI ANSWER?

### ‚úÖ Medical Questions:
- Drug dosages and interactions
- Disease explanations
- Treatment protocols
- Diagnosis help
- WHO guidelines

### ‚úÖ General Knowledge:
- Science and biology
- How things work
- Medical research
- Patient education
- Medical calculations

### ‚úÖ Clinical Support:
- Case discussions
- Differential diagnosis
- Treatment options
- Risk assessment
- Evidence-based medicine

### ‚úÖ Non-Medical Too!
- General questions
- Explanations
- Problem-solving
- Advice and recommendations

---

## üéØ CURRENT CONFIGURATION

**File**: `/server/routes/lumina-ai-local.js`

```javascript
// Line 11: Model selection
const DEFAULT_MODEL = 'llama2';  // üî• REAL AI - answers ANYTHING!

// Lines 95-110: AI settings
{
  model: DEFAULT_MODEL,
  prompt: fullPrompt,
  stream: false,
  options: {
    temperature: 0.8,   // Higher = more creative
    top_p: 0.95,        // Diversity of responses
    max_tokens: 800     // Max response length
  }
}
```

---

## üöÄ HOW IT WORKS

1. **Doctor asks a question** in Alt-X
2. Backend sends question to **Ollama** (running locally on port 11434)
3. **Llama 2 AI model** generates the answer
4. Answer is sent back to the doctor

**No internet needed! No API costs! 100% Private!**

---

## üîß TROUBLESHOOTING

### AI not responding?
```bash
# Check if Ollama is running
ps aux | grep ollama

# If not running, start it
ollama serve
```

### Want faster responses?
- Switch to `tinyllama` (3 seconds vs 10 seconds)
- Trade-off: Less accurate answers

### Want better answers?
- Switch to `llama2` or `mistral`
- Trade-off: Slower responses

### Model not found error?
```bash
# List installed models
ollama list

# Pull a missing model
ollama pull llama2
```

---

## üìä MODEL COMPARISON

| Model | Size | Speed | Quality | Best For |
|-------|------|-------|---------|----------|
| **Llama 2** | 3.8GB | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Complex medical questions |
| **Mistral** | 4.4GB | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Reasoning & logic |
| **TinyLlama** | 637MB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Quick simple answers |
| **Llama 3** | 4.7GB | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Best overall (recommended!) |

---

## üéì RECOMMENDED SETUP FOR DOCTORS

**For Hospital/Clinic Use:**
```javascript
const DEFAULT_MODEL = 'llama2';  // Best balance of speed & quality
```

**For Quick Consultations:**
```javascript
const DEFAULT_MODEL = 'tinyllama';  // Fast responses
```

**For Complex Cases:**
```javascript
const DEFAULT_MODEL = 'mistral';  // Deep reasoning
```

---

## üåü YOU NOW HAVE:

‚úÖ **REAL AI** - Not pre-programmed responses!  
‚úÖ **FREE** - No API costs, runs locally  
‚úÖ **PRIVATE** - Data never leaves your server  
‚úÖ **POWERFUL** - Can answer ANY question  
‚úÖ **FAST** - 3-10 seconds per response  
‚úÖ **FLEXIBLE** - Switch models anytime  

**Alt-X is now a TRUE AI assistant powered by Llama 2! üöÄ**
